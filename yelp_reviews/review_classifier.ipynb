{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\deept\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import itertools\n",
    "import operator\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\deept\\Anaconda3\\lib\\site-packages\\nltk\\twitter\\__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.read_csv('yelp__review.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0      object\n",
       "review_id       object\n",
       "user_id         object\n",
       "business_id     object\n",
       "stars          float64\n",
       "date            object\n",
       "text            object\n",
       "useful         float64\n",
       "funny          float64\n",
       "cool           float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_df = reviews[['business_id','text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "business_id    5996997\n",
       "text           5996994\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "business_id     188595\n",
       "text           5992993\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_df.groupby('business_id')\n",
    "reviews_df.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be observed that there are just **188595** business ids while **5992993** texts, which implies the dataset consists of multiple reviews given by different users for the same business. Review text has to now be grouped based on the business id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "business_id\n",
      "--1UhMGODdWsrMastO9DZw      24\n",
      "--6MefnULPED_I942VcFNA      39\n",
      "--7zmmkVg-IMGaXbuVd0SQ      54\n",
      "--8LPVSo5i0Oo61X01sV9A       4\n",
      "--9QQLMTbFzLJ_oT-ON3Xw      11\n",
      "--9e1ONYQuAa-CB_Rrw7Tw    1545\n",
      "--DaPTJW3-tB1vP-PfdTEg      45\n",
      "--DdmeR16TRb3LsjG0ejrQ       5\n",
      "--EF5N7P70J_UYBTPypYlA       5\n",
      "--EX4rRznJrltyn-34Jz1w       4\n",
      "--FBCX-N37CMYDfs790Bnw     125\n",
      "--FLdgM0GNpXVMn74ppCGw      12\n",
      "--GM_ORV2cYS-h38DSaCLw       8\n",
      "--Gc998IMjLn8yr-HTzGUg       3\n",
      "--I7YYLada0tSLkORTHb5Q      83\n",
      "--KCl2FvVQpvjzmZSPyviA      13\n",
      "--KQsXc-clkO7oHRqGzSzg      35\n",
      "--Ni3oJ4VOqfOEu7Sj2Vzg       6\n",
      "--Rsj71PBe31h5YljVseKA       8\n",
      "--S62v0QgkqQaVUhFnNHrw      35\n",
      "--SrzpvFLwP_YFwB_Cetow      44\n",
      "--TcDRzRIxhvHM4DSgEuMA       5\n",
      "--U98MNlDym2cLn36BBPgQ       4\n",
      "--VMPfs4zfZJtQbqzJsNhg      10\n",
      "--WsruI0IGEoeRmkErU5Gg      15\n",
      "--Y7NhBKzLTbNliMUX_wfg      12\n",
      "--YPwqIlRJrhHkJcjY3eiA       3\n",
      "--ab39IjZR_xUf81WyTyHg      11\n",
      "--cZ6Hhc9F7VkKXxHMVZSQ     333\n",
      "--cgVkbWTiga3OYTkymKqA      13\n",
      "                          ... \n",
      "zzXuJF6UUT1kgTyCsmgDmQ       4\n",
      "zzYaAiC0rLNSDiFQlMKOEQ       5\n",
      "zzYoocdehksv9_tg80a97w       3\n",
      "zzZfgEpwrpi4Ywdaj3OIuQ      24\n",
      "zzZuGP5tmiZYxa52A3C5yQ      12\n",
      "zza5aje9ElBdWOO3Qtfk_Q       3\n",
      "zzafWwri6ywvhvMsjS-l6w       4\n",
      "zzbsLej_8KtTxGwJdOb09w       7\n",
      "zze6IysT7bJFS8gvi6fZ2A      20\n",
      "zzf3RkMI1Y2E1QaZqeU8yA      34\n",
      "zzgSiOnuUjnBnmfR-ZG4ww       4\n",
      "zzgdTix-4eDMxtiyIhesMw       9\n",
      "zzguctNQ68aKoe3upWaimw       9\n",
      "zzjKekzQ6i4iR-qpo405Pw      15\n",
      "zzjqFOujmM9surbMANZ_ag       4\n",
      "zzkLY0npjBFX3gxGitmbUA       3\n",
      "zzlZJVkEhOzR2tJOLHcF2A      40\n",
      "zzm9aR0UHEEEYdsR_zKcuw       5\n",
      "zzmIMvqiBJ_-wVKg_OnGpw     110\n",
      "zzsKbL1KMNJqazSqBXskxQ       6\n",
      "zzsOLFhgUw8gnjLTVVItFA      97\n",
      "zzsU528uoRB6qZUGhKDa6w       7\n",
      "zzt8omL4h_tdLYpY_rlZ4w      10\n",
      "zztHqd_3ULxuIS8lkhYdwg       5\n",
      "zzuOCWxuY39YJ1wnTwQ0Lg       3\n",
      "zzvlwkcNR1CCqOPXwuvz2A       5\n",
      "zzwaS0xn1MVEPEf0hNLjew      68\n",
      "zzwhN7x37nyjP0ZM8oiHmw      44\n",
      "zzwicjPC9g246MK2M1ZFBA      63\n",
      "zzzaIBwimxVej4tY6qFOUQ      38\n",
      "Name: text, Length: 188595, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = reviews_df.groupby('business_id')['text'].nunique()\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_ = reviews_df.set_index('business_id')['text'].to_dict()\n",
    "dict_ = {k: list(v) for k,v in reviews_df.groupby('business_id')['text']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once data is grouped, it is of the form **business_id:[review1, review2, ..., reviewN]**. Find the sentiment of each of the reviews and store it as a **dict[business_id]: [sent1, sent2, sent3..., sentN]**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 10000 10000\n",
      "10000 20000 20000\n",
      "20000 30000 30000\n",
      "30000 40000 40000\n",
      "40000 50000 50000\n",
      "50000 60000 60000\n",
      "60000 70000 70000\n",
      "70000 80000 80000\n",
      "80000 90000 90000\n",
      "90000 100000 100000\n",
      "100000 110000 110000\n",
      "110000 120000 120000\n",
      "120000 130000 130000\n",
      "130000 140000 140000\n",
      "140000 150000 150000\n",
      "150000 160000 160000\n",
      "160000 170000 170000\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "upper_limit = 0\n",
    "lower_limit = 0\n",
    "sent = {}\n",
    "while count!=170000:\n",
    "    lower_limit=upper_limit\n",
    "    upper_limit+=10000\n",
    "    x = itertools.islice(dict_.items(), lower_limit,upper_limit)\n",
    "    for key,val in x:\n",
    "        count += 1\n",
    "        classify = []\n",
    "#         if key not in sent:\n",
    "#             sent[key] = []\n",
    "        try:\n",
    "            for val_item in val:\n",
    "                sentiment = sentiment_analyzer.polarity_scores(val_item)\n",
    "                sentiment.pop('compound')\n",
    "                sentiment.pop('neu')\n",
    "                classify.append(max(sentiment.items(), key=operator.itemgetter(1))[0])\n",
    "            sent[key] = classify\n",
    "        except:\n",
    "            continue\n",
    "    print(lower_limit, upper_limit, count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = itertools.islice(dict_.items(),170001,188595)\n",
    "for key,val in x:\n",
    "    count += 1\n",
    "    classify = []\n",
    "#         if key not in sent:\n",
    "#             sent[key] = []\n",
    "    try:\n",
    "        for val_item in val:\n",
    "            sentiment = sentiment_analyzer.polarity_scores(val_item)\n",
    "            sentiment.pop('compound')\n",
    "            sentiment.pop('neu')\n",
    "            classify.append(max(sentiment.items(), key=operator.itemgetter(1))[0])\n",
    "        sent[key] = classify\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Find the max occurence of the sentiment, that determines if the overall feedback is pos or neg__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('sentiment_dict.csv','w',newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    for k,v in sent.items():\n",
    "        writer.writerow([k,v])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function TextIOWrapper.close>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "json = json.dumps(sent)\n",
    "f = open(\"sentiment_dict.json\",\"w\")\n",
    "f.write(json)\n",
    "f.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_class = {}\n",
    "for k,v in sent.items():\n",
    "#     print(k,v)\n",
    "    pos_neg = {'pos':0, 'neg':0}\n",
    "    for val in v:\n",
    "#         print(val)\n",
    "        if val == 'pos':\n",
    "            pos_neg['pos']+=1\n",
    "        else:\n",
    "            pos_neg['neg']+=1\n",
    "    print(pos_neg)\n",
    "    final_class[k] = max(pos_neg.items(), key=operator.itemgetter(1))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Business_id and collabarative sentiment of the review texts:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Final count of positive and negative ratings:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "172476"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(1 for x in final_class.values() if x == 'pos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16115"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(1 for x in final_class.values() if x == 'neg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = csv.writer(open(\"review_classification.csv\",\"w\"))\n",
    "for key, val  in final_class.items():\n",
    "    w.writerow([key, val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function TextIOWrapper.close>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "json = json.dumps(final_class)\n",
    "f = open(\"review_classification.json\",\"w\")\n",
    "f.write(json)\n",
    "f.close"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
